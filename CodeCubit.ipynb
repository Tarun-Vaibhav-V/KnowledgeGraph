{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS3LQidnOCxX",
        "outputId": "b4b0a287-6afd-4831-829e-d41aa931e62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: node2vec in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.3.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nebula3-python in /usr/local/lib/python3.11/dist-packages (3.8.3)\n",
            "Requirement already satisfied: future>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from nebula3-python) (1.0.0)\n",
            "Requirement already satisfied: httplib2>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from nebula3-python) (0.22.0)\n",
            "Requirement already satisfied: pytz>=2021.1 in /usr/local/lib/python3.11/dist-packages (from nebula3-python) (2025.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from nebula3-python) (1.17.0)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.22.0->nebula3-python) (0.28.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.20.0->nebula3-python) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.22.0->nebula3-python) (4.2.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.22.0->nebula3-python) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.22.0->nebula3-python) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->httpx[http2]>=0.22.0->nebula3-python) (4.14.0)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install spacy networkx node2vec torch transformers requests beautifulsoup4\n",
        "!pip install nebula3-python\n",
        "!python -m spacy download en_core_web_sm\n",
        "# Import essential libraries\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from node2vec import Node2Vec\n",
        "import requests\n",
        "import re\n",
        "from collections import defaultdict, deque\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "import json\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataCollector:\n",
        "    \"\"\"Downloads and preprocesses War and Peace from Project Gutenberg\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
        "        self.text = None\n",
        "\n",
        "    def download_text(self):\n",
        "        \"\"\"Download War and Peace text\"\"\"\n",
        "        try:\n",
        "            response = requests.get(self.url)\n",
        "            response.raise_for_status()\n",
        "            self.text = response.text\n",
        "            print(\"‚úÖ Successfully downloaded War and Peace!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading text: {e}\")\n",
        "            # Fallback sample text for demonstration\n",
        "            self.text = \"\"\"\n",
        "            Anna Pavlovna smiled and promised to take care of Pierre. She knew his father.\n",
        "            The guests began to disperse, some without taking leave of Anna Pavlovna.\n",
        "            He was taller than anyone in the room. His expression was intelligent and his gaze was kind.\n",
        "            \"\"\"\n",
        "            return False\n",
        "\n",
        "    def clean_text(self):\n",
        "        \"\"\"Clean and preprocess the text\"\"\"\n",
        "        if not self.text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove Project Gutenberg header/footer\n",
        "        start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        start_idx = self.text.find(start_marker)\n",
        "        end_idx = self.text.find(end_marker)\n",
        "\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            self.text = self.text[start_idx:end_idx]\n",
        "\n",
        "        # Basic cleaning\n",
        "        self.text = re.sub(r'\\n+', ' ', self.text)  # Replace multiple newlines\n",
        "        self.text = re.sub(r'\\s+', ' ', self.text)  # Normalize whitespace\n",
        "        self.text = self.text.strip()\n",
        "\n",
        "        return self.text[:50000]  # Use first 50k characters for demo\n",
        "\n",
        "\n",
        "# ‚úÖ Initialize and run data collection\n",
        "collector = DataCollector()\n",
        "collector.download_text()\n",
        "clean_text = collector.clean_text()\n",
        "print(f\"üìä Text length: {len(clean_text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ho4AvpBODxu",
        "outputId": "36ccdbb2-81e9-437f-d0bf-090a4e21df48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully downloaded War and Peace!\n",
            "üìä Text length: 50000 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SVOExtractor:\n",
        "    \"\"\"Extracts Subject-Verb-Object triplets using spaCy\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.triplets = []\n",
        "        self.sentence_mapping = {}  # Maps triplets to sentence IDs\n",
        "\n",
        "    def extract_subject(self, token):\n",
        "        \"\"\"Extract subject from dependency tree\"\"\"\n",
        "        for child in token.children:\n",
        "            if \"subj\" in child.dep_:\n",
        "                return \" \".join([w.text for w in child.subtree])\n",
        "        return None\n",
        "\n",
        "    def extract_object(self, token):\n",
        "        \"\"\"Extract object from dependency tree\"\"\"\n",
        "        for child in token.children:\n",
        "            if \"obj\" in child.dep_:\n",
        "                return \" \".join([w.text for w in child.subtree])\n",
        "        return None\n",
        "\n",
        "    def extract_triplets_from_sentence(self, sentence, sentence_id):\n",
        "        \"\"\"Extract SVO triplets from a single sentence\"\"\"\n",
        "        doc = self.nlp(sentence)\n",
        "        sentence_triplets = []\n",
        "\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"VERB\":\n",
        "                subject = self.extract_subject(token)\n",
        "                obj = self.extract_object(token)\n",
        "                verb = token.lemma_\n",
        "                if subject and obj:\n",
        "                    triplet = (subject.lower().strip(), verb.lower(), obj.lower().strip())\n",
        "                    sentence_triplets.append(triplet)\n",
        "                    self.sentence_mapping[triplet] = sentence_id\n",
        "        return sentence_triplets\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"Process entire text and extract all triplets\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents]\n",
        "        all_triplets = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences[:100]):  # Limit for demo\n",
        "            triplets = self.extract_triplets_from_sentence(sentence, i)\n",
        "            all_triplets.extend(triplets)\n",
        "\n",
        "        self.triplets.extend(all_triplets)\n",
        "        print(f\"üîç Extracted {len(all_triplets)} SVO triplets from {len(sentences)} sentences\")\n",
        "        return all_triplets, sentences\n",
        "\n",
        "\n",
        "# ‚úÖ Extract SVO triplets\n",
        "extractor = SVOExtractor()\n",
        "triplets, sentences = extractor.process_text(clean_text)\n",
        "\n",
        "print(\"üìù Sample triplets:\")\n",
        "for i, triplet in enumerate(triplets[:5]):\n",
        "    print(f\" {i+1}. {triplet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMY31GbBOS4U",
        "outputId": "8501b132-65c8-45e9-bc5c-af4e58c8d6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Extracted 52 SVO triplets from 462 sentences\n",
            "üìù Sample triplets:\n",
            " 1. ('chapter xxiv chapter xxv chapter xxvi', 'book', 'seven')\n",
            " 2. ('i', 'warn', 'you')\n",
            " 3. ('you', 'tell', 'me')\n",
            " 4. ('this', 'mean', 'war')\n",
            " 5. ('i', 'have', 'nothing more to do with you')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraphBuilder:\n",
        "    \"\"\"Builds knowledge graph from SVO triplets\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "        self.node_to_sentences = defaultdict(set)\n",
        "        self.edge_to_sentences = defaultdict(set)\n",
        "\n",
        "    def build_graph(self, triplets, sentence_mapping):\n",
        "        \"\"\"Build NetworkX graph from triplets\"\"\"\n",
        "        for triplet in triplets:\n",
        "            subject, verb, obj = triplet\n",
        "            sentence_id = sentence_mapping.get(triplet, -1)\n",
        "\n",
        "            # Add nodes\n",
        "            self.graph.add_node(subject, type='entity')\n",
        "            self.graph.add_node(obj, type='entity')\n",
        "\n",
        "            # Add edge with verb as relationship\n",
        "            self.graph.add_edge(subject, obj, relation=verb, sentence_id=sentence_id)\n",
        "\n",
        "            # Track sentence membership\n",
        "            self.node_to_sentences[subject].add(sentence_id)\n",
        "            self.node_to_sentences[obj].add(sentence_id)\n",
        "            self.edge_to_sentences[(subject, obj)].add(sentence_id)\n",
        "\n",
        "        print(f\"üï∏Ô∏è Built graph with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges\")\n",
        "        return self.graph\n",
        "\n",
        "    def get_node_features(self):\n",
        "        \"\"\"Generate node features for entropy calculation\"\"\"\n",
        "        features = {}\n",
        "        for node in self.graph.nodes():\n",
        "            features[node] = {\n",
        "                'degree': self.graph.degree(node),\n",
        "                'in_degree': self.graph.in_degree(node),\n",
        "                'out_degree': self.graph.out_degree(node),\n",
        "                'sentence_count': len(self.node_to_sentences[node]),\n",
        "                'neighbors': len(list(self.graph.neighbors(node)))\n",
        "            }\n",
        "        return features\n",
        "\n",
        "\n",
        "# ‚úÖ Build knowledge graph\n",
        "kg_builder = KnowledgeGraphBuilder()\n",
        "knowledge_graph = kg_builder.build_graph(triplets, extractor.sentence_mapping)\n",
        "node_features = kg_builder.get_node_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv4_dFtmOTLu",
        "outputId": "44473a00-5f48-4157-d92c-78e6158c665a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üï∏Ô∏è Built graph with 68 nodes and 50 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EntropyModel(nn.Module):\n",
        "    \"\"\"BLT-inspired entropy model for word-level boundary detection\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=10000, embedding_dim=128, hidden_dim=256):\n",
        "        super(EntropyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.entropy_head = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        \"\"\"Calculate entropy for input sequence\"\"\"\n",
        "        embedded = self.embedding(input_sequence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        entropy_scores = self.sigmoid(self.entropy_head(lstm_out))\n",
        "        return entropy_scores\n",
        "\n",
        "    def calculate_sequence_entropy(self, node_sequence):\n",
        "        \"\"\"Calculate entropy for a sequence of nodes\"\"\"\n",
        "        # Convert nodes to indices (simplified tokenization)\n",
        "        node_to_idx = {node: i for i, node in enumerate(set(node_sequence))}\n",
        "        sequence_indices = torch.tensor([node_to_idx[node] for node in node_sequence])\n",
        "\n",
        "        if len(sequence_indices.shape) == 1:\n",
        "            sequence_indices = sequence_indices.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        with torch.no_grad():\n",
        "            entropy_scores = self.forward(sequence_indices)\n",
        "        return entropy_scores.squeeze().numpy()\n",
        "\n",
        "\n",
        "class GraphTraverser:\n",
        "    \"\"\"Implements graph traversal with entropy-based stopping\"\"\"\n",
        "\n",
        "    def __init__(self, graph, entropy_model, threshold=0.7):\n",
        "        self.graph = graph\n",
        "        self.entropy_model = entropy_model\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def traverse_from_node(self, start_node, max_steps=10):\n",
        "        \"\"\"Traverse graph from starting node until entropy threshold\"\"\"\n",
        "        visited = set()\n",
        "        path = [start_node]\n",
        "        current_node = start_node\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            visited.add(current_node)\n",
        "\n",
        "            # Get unvisited neighbors\n",
        "            neighbors = list(self.graph.neighbors(current_node))\n",
        "            unvisited_neighbors = [n for n in neighbors if n not in visited]\n",
        "\n",
        "            if not unvisited_neighbors:\n",
        "                break\n",
        "\n",
        "            # Calculate entropy\n",
        "            if len(path) > 2:\n",
        "                entropy_scores = self.entropy_model.calculate_sequence_entropy(path)\n",
        "                current_entropy = np.mean(entropy_scores)\n",
        "\n",
        "                if current_entropy > self.threshold:\n",
        "                    print(f\"üõë Stopping traversal at entropy {current_entropy:.3f}\")\n",
        "                    break\n",
        "\n",
        "            # Move to next node\n",
        "            next_node = unvisited_neighbors[0]  # You could use [-1] or random.choice(...)\n",
        "            path.append(next_node)\n",
        "            current_node = next_node\n",
        "\n",
        "        return path, visited\n",
        "\n",
        "\n",
        "# ‚úÖ Initialize model and traverser\n",
        "entropy_model = EntropyModel()\n",
        "traverser = GraphTraverser(knowledge_graph, entropy_model)"
      ],
      "metadata": {
        "id": "Qu4APVINOTQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NebulaGraphConnector:\n",
        "    \"\"\"Handles NebulaGraph database operations (simulated version)\"\"\"\n",
        "\n",
        "    def __init__(self, host='127.0.0.1', port=9669, username='root', password='nebula'):\n",
        "        # Note: This assumes NebulaGraph is running locally\n",
        "        # For Colab, you'd need to set up NebulaGraph in Docker\n",
        "        self.config = {\n",
        "            'host': host,\n",
        "            'port': port,\n",
        "            'username': username,\n",
        "            'password': password\n",
        "        }\n",
        "        self.connected = False\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Connect to NebulaGraph (placeholder for actual connection)\"\"\"\n",
        "        try:\n",
        "            # In a real implementation, you'd use nebula3-python here\n",
        "            print(\"üîå NebulaGraph connection simulated (use Docker setup for real deployment)\")\n",
        "            self.connected = True\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Connection failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_schema(self):\n",
        "        \"\"\"Create graph schema\"\"\"\n",
        "        schema_commands = [\n",
        "            \"CREATE SPACE IF NOT EXISTS sentence_kg(vid_type=FIXED_STRING(256));\",\n",
        "            \"USE sentence_kg;\",\n",
        "            \"CREATE TAG IF NOT EXISTS entity(name string);\",\n",
        "            \"CREATE EDGE IF NOT EXISTS relation(verb string, sentence_id int);\"\n",
        "        ]\n",
        "        print(\"üìã Schema creation simulated\")\n",
        "        return schema_commands\n",
        "\n",
        "    def insert_graph_data(self, graph):\n",
        "        \"\"\"Insert graph data into NebulaGraph\"\"\"\n",
        "        print(f\"üíæ Simulating insertion of {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
        "        # In a real implementation, you'd batch insert the data using NebulaGraph client\n",
        "        return True\n",
        "\n",
        "\n",
        "# ‚úÖ Setup NebulaGraph connection (simulated)\n",
        "nebula_conn = NebulaGraphConnector()\n",
        "nebula_conn.connect()\n",
        "nebula_conn.create_schema()\n",
        "nebula_conn.insert_graph_data(knowledge_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwyOpyI2OTUQ",
        "outputId": "d8ebb90e-2153-4352-dd9f-8bffb6a64843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîå NebulaGraph connection simulated (use Docker setup for real deployment)\n",
            "üìã Schema creation simulated\n",
            "üíæ Simulating insertion of 68 nodes and 50 edges\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BoundaryDetectionEvaluator:\n",
        "    \"\"\"Evaluates sentence boundary detection performance\"\"\"\n",
        "\n",
        "    def __init__(self, kg_builder, sentences):\n",
        "        self.kg_builder = kg_builder\n",
        "        self.sentences = sentences\n",
        "        self.ground_truth = self._create_ground_truth()\n",
        "\n",
        "    def _create_ground_truth(self):\n",
        "        \"\"\"Create ground truth sentence groupings\"\"\"\n",
        "        ground_truth = {}\n",
        "        for node in self.kg_builder.graph.nodes():\n",
        "            sentence_ids = list(self.kg_builder.node_to_sentences[node])\n",
        "            ground_truth[node] = sentence_ids\n",
        "        return ground_truth\n",
        "\n",
        "    def evaluate_traversal(self, start_nodes, traverser):\n",
        "        \"\"\"Evaluate boundary detection performance\"\"\"\n",
        "        predictions = {}\n",
        "        for start_node in start_nodes:\n",
        "            if start_node in self.kg_builder.graph.nodes():\n",
        "                path, visited = traverser.traverse_from_node(start_node)\n",
        "                predicted_sentences = set()\n",
        "                for node in visited:\n",
        "                    if node in self.kg_builder.node_to_sentences:\n",
        "                        predicted_sentences.update(self.kg_builder.node_to_sentences[node])\n",
        "                predictions[start_node] = list(predicted_sentences)\n",
        "        return self._calculate_metrics(predictions)\n",
        "\n",
        "    def _calculate_metrics(self, predictions):\n",
        "        \"\"\"Calculate F1, precision, recall\"\"\"\n",
        "        all_true = []\n",
        "        all_pred = []\n",
        "        for node, predicted_sentences in predictions.items():\n",
        "            if node in self.ground_truth:\n",
        "                true_sentences = set(self.ground_truth[node])\n",
        "                pred_sentences = set(predicted_sentences)\n",
        "                max_sentence_id = max([\n",
        "                    max(ids) if ids else 0\n",
        "                    for ids in self.ground_truth.values()\n",
        "                ])\n",
        "                for sent_id in range(max_sentence_id + 1):\n",
        "                    all_true.append(1 if sent_id in true_sentences else 0)\n",
        "                    all_pred.append(1 if sent_id in pred_sentences else 0)\n",
        "\n",
        "        if all_true and all_pred:\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                all_true, all_pred, average='binary', zero_division=0\n",
        "            )\n",
        "            return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "        else:\n",
        "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "\n",
        "# ‚úÖ Run evaluation\n",
        "evaluator = BoundaryDetectionEvaluator(kg_builder, sentences)\n",
        "start_nodes = list(knowledge_graph.nodes())[:5]  # Test with first 5 nodes\n",
        "metrics = evaluator.evaluate_traversal(start_nodes, traverser)\n",
        "\n",
        "print(\"üìä Evaluation Results:\")\n",
        "print(f\" Precision: {metrics['precision']:.3f}\")\n",
        "print(f\" Recall:    {metrics['recall']:.3f}\")\n",
        "print(f\" F1-Score:  {metrics['f1']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rChm4O9cOTXB",
        "outputId": "9793b405-7a9f-4b2b-d2a3-6bf8aedcf60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluation Results:\n",
            " Precision: 0.724\n",
            " Recall:    1.000\n",
            " F1-Score:  0.840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_pipeline():\n",
        "    \"\"\"Run the complete entropy-based boundary detection pipeline\"\"\"\n",
        "    print(\"üöÄ Starting Entropy-Based Sentence Boundary Detection Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Data Collection\n",
        "    print(\"\\nüì• Step 1: Data Collection\")\n",
        "    collector = DataCollector()\n",
        "    collector.download_text()\n",
        "    clean_text = collector.clean_text()\n",
        "\n",
        "    # Step 2: SVO Extraction\n",
        "    print(\"\\nüîç Step 2: SVO Triplet Extraction\")\n",
        "    extractor = SVOExtractor()\n",
        "    triplets, sentences = extractor.process_text(clean_text)\n",
        "\n",
        "    # Step 3: Knowledge Graph Construction\n",
        "    print(\"\\nüï∏Ô∏è Step 3: Knowledge Graph Construction\")\n",
        "    kg_builder = KnowledgeGraphBuilder()\n",
        "    knowledge_graph = kg_builder.build_graph(triplets, extractor.sentence_mapping)\n",
        "\n",
        "    # Step 4: Entropy Model Training (simplified)\n",
        "    print(\"\\nüß† Step 4: Entropy Model Initialization\")\n",
        "    entropy_model = EntropyModel()\n",
        "    traverser = GraphTraverser(knowledge_graph, entropy_model)\n",
        "\n",
        "    # Step 5: Boundary Detection\n",
        "    print(\"\\nüéØ Step 5: Boundary Detection\")\n",
        "    start_nodes = list(knowledge_graph.nodes())[:5]\n",
        "    results = {}\n",
        "    for node in start_nodes:\n",
        "        path, visited = traverser.traverse_from_node(node)\n",
        "        results[node] = {'path': path, 'visited': list(visited)}\n",
        "        print(f\" From '{node}': visited {len(visited)} nodes\")\n",
        "\n",
        "    # Step 6: Evaluation\n",
        "    print(\"\\nüìä Step 6: Evaluation\")\n",
        "    evaluator = BoundaryDetectionEvaluator(kg_builder, sentences)\n",
        "    metrics = evaluator.evaluate_traversal(start_nodes, traverser)\n",
        "\n",
        "    print(f\"\\nüéâ Final Results:\")\n",
        "    print(f\" Nodes in Graph: {knowledge_graph.number_of_nodes()}\")\n",
        "    print(f\" Edges in Graph: {knowledge_graph.number_of_edges()}\")\n",
        "    print(f\" F1-Score: {metrics['f1']:.3f}\")\n",
        "    print(f\" Precision: {metrics['precision']:.3f}\")\n",
        "    print(f\" Recall: {metrics['recall']:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'graph': knowledge_graph,\n",
        "        'metrics': metrics,\n",
        "        'results': results,\n",
        "        'sentences': sentences\n",
        "    }\n",
        "\n",
        "# Run the complete pipeline\n",
        "pipeline_results = run_complete_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQz8KTVcOTZy",
        "outputId": "65af43f0-d4ac-441d-ea71-3f406f8f4bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Entropy-Based Sentence Boundary Detection Pipeline\n",
            "============================================================\n",
            "\n",
            "üì• Step 1: Data Collection\n",
            "‚úÖ Successfully downloaded War and Peace!\n",
            "\n",
            "üîç Step 2: SVO Triplet Extraction\n",
            "üîç Extracted 52 SVO triplets from 462 sentences\n",
            "\n",
            "üï∏Ô∏è Step 3: Knowledge Graph Construction\n",
            "üï∏Ô∏è Built graph with 68 nodes and 50 edges\n",
            "\n",
            "üß† Step 4: Entropy Model Initialization\n",
            "\n",
            "üéØ Step 5: Boundary Detection\n",
            " From 'chapter xxiv chapter xxv chapter xxvi': visited 2 nodes\n",
            " From 'seven': visited 1 nodes\n",
            " From 'i': visited 3 nodes\n",
            " From 'you': visited 2 nodes\n",
            " From 'me': visited 1 nodes\n",
            "\n",
            "üìä Step 6: Evaluation\n",
            "\n",
            "üéâ Final Results:\n",
            " Nodes in Graph: 68\n",
            " Edges in Graph: 50\n",
            " F1-Score: 0.840\n",
            " Precision: 0.724\n",
            " Recall: 1.000\n"
          ]
        }
      ]
    }
  ]
}